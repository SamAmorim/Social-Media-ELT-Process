{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92fcd8e7-1aa7-43c3-b91e-e6c5a5e44d2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd \n",
    "\n",
    "schema_nome = \"jsonplaceholder_copper\"\n",
    "tabela_nome = \"posts_copper\"\n",
    "endereco_tabela = f\"{schema_nome}.{tabela_nome}\"\n",
    "url = 'https://jsonplaceholder.typicode.com/posts'\n",
    "\n",
    "try:\n",
    "    resposta = requests.get(url)\n",
    "    resposta.raise_for_status()\n",
    "    dados = resposta.json()\n",
    "\n",
    "    print(f\"Requisição bem-sucedida! Foram encontrados {len(dados)} posts.\")\n",
    "\n",
    "    print(json.dumps(dados[0], indent=2))\n",
    "\n",
    "    # Gerar um DataFrame pandas\n",
    "    df_pandas = pd.DataFrame(dados)\n",
    "\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Erro ao fazer a requisição: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d75509b-bc00-4485-af41-da1cbf2b4def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "\n",
    "print(\"caminho: \" + endereco_tabela)\n",
    "\n",
    "df_spark = spark.createDataFrame(df_pandas)\n",
    "\n",
    "df_spark.show()\n",
    "df_spark.printSchema()\n",
    "df_spark.write.mode(\"overwrite\").saveAsTable(endereco_tabela)\n",
    "df_verificacao = spark.read.table(endereco_tabela)\n",
    "spark.sql(f\"GRANT ALL PRIVILEGES ON TABLE {endereco_tabela} TO `account users`\")\n",
    "df_verificacao.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6643852837098592,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "jsonplaceholder_posts_copper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
